{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Building a Convolutional Neural Network from Scratch using Numpy</center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are two components of CNN:\n",
    "- Convolutional layer\n",
    "- Pooling layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A convolutional layer consists of a set of filters (also called kernels) that when applied to the layerâ€™s input perform some kind of modification of the original image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"./images/CNN-from-Scratch1.png\" width=\"500\" />](./images/CNN-from-Scratch1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the above image, a 3x3 kernel is used.\n",
    "- The value of the elements in the kernels are not to be chosen manually but are parameters that the network learns during training.\n",
    "- The role of convolutions is to isolate different features present in the image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling Layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The task of a pooling layer is to shrink the input images to reduce the computational load and memory consumption of the network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"./images/CNN-from-Scratch2.gif\" width=\"500\" />](./images/CNN-from-Scratch2.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Max Pooling, the 2x2 max pooling kernel takes 4 pixels of the input image and returns only the pixel with the maximum value.\n",
    "- In Average Pooling, the 2x2 max pooling kernel takes 4 pixels of the input image and returns only the pixel with the average value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoery of some other processes in CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"./images/CNN-from-Scratch3.png\" width=\"500\" />](./images/CNN-from-Scratch3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we have to keep the size of kernel same in the all process because in convolutional operations, the size of input image decreases. \n",
    "- padding preserves the input size.\n",
    "- in the above example, the input size was (36x36x3) but afer convolutional operations, the size decreased to (32x32x3) and we have preserved that by padding as shown above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flattening"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"./images/CNN-from-Scratch4.png\" width=\"500\" />](./images/CNN-from-Scratch4.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- flattening is basically converting the output matrix of convolutional and pooling layer to one dimensional array.\n",
    "- we do this because after flatening, the matrix is passed to ANN (Artifical Neural Network) which consists of one dimensional array."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"./images/CNN-from-Scratch5.png\" width=\"500\" />](./images/CNN-from-Scratch5.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in dropout, we drop nodes of visible or hidden layers in neural network.\n",
    "- dropout is basically a regularization technique for reducing overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stride"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"./images/CNN-from-Scratch6.png\" width=\"500\" />](./images/CNN-from-Scratch6.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stride is the number of pixels shifts over the input matrix.\n",
    "- when stride is 1, the filter moves by 1 pixel every time.\n",
    "- similarly when stride is 2, the filter moves by 2 pixel every time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully Connected Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"./images/CNN-from-Scratch7.png\" width=\"500\" />](./images/CNN-from-Scratch7.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in a fully connected layer, the input layer nodes are connected to every node in the second layer.\n",
    "- these fully connected layers are usually used at the end of the CNN.\n",
    "- activation functions and dropout layers are used between two fully connected layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"./images/CNN-from-Scratch8.png\" width=\"750\" />](./images/CNN-from-Scratch8.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "    \n",
    "    # constructor to initializa the convolutional layer with number of kernels, size of kernel and random filter of shape (kernel_sum, kernel_size, kernel_size)\n",
    "    def __init__(self, kernel_num, kernel_shape):\n",
    "        self.kernel_num = kernel_num # number of kernels of convolutional layer\n",
    "        self.kernel_shape = kernel_shape # size of the kernel\n",
    "        self.kernels = np.random.rand(kernel_num, kernel_shape, kernel_shape)/(kernel_shape**2) # we have divided by the squared of kernel size for normalization\n",
    "       \n",
    "    # generate patches of input image whose shape depends on kernel size\n",
    "    def patches_generator(self, input_image):\n",
    "        height_image, width_image = input_image.shape\n",
    "        self.input_image = input_image\n",
    "        for h in range(height_image - self.kernel_shape + 1):\n",
    "            for w in range(width_image - self.kernel_shape + 1):\n",
    "                patch = input_image[h:(h+self.kernel_shape), w:(w+self.kernel_shape)]\n",
    "                yield patch, h, w\n",
    "                \n",
    "    # forward propogation after generating the patches of the input image i.e. it carries the convolution of each image\n",
    "    def forward_propogation(self, input_image):\n",
    "        height_image, width_image = input_image.shape\n",
    "        convolution_output = np.zeros((height_image - self.kernel_shape + 1, width_image - self.kernel_shape + 1, self.kernel_num))\n",
    "        for patch, h, w in self.patches_generator(input_image=input_image):\n",
    "            convolution_output[h, w] = np.sum(patch*self.kernels, axis=(1, 2))\n",
    "        return convolution_output\n",
    "    \n",
    "    # back propogation responsible for finding the gradiant of loss function w.r.t each weight of the layer\n",
    "    def back_propogation(self, dE_dY, alpha):\n",
    "        dE_dk = np.zeros(self.kernels.shape) # initializing the array of gradiant of loss function w.r.t each weight\n",
    "        for patch, h, w in self.patches_generator(self.image):\n",
    "            for f in range(self.kernel_num):\n",
    "                dE_dk[f] += patch*dE_dY[h, w, f] \n",
    "        self.kernels -= alpha*dE_dk # updating the weight's value\n",
    "        return dE_dk\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolingLayer:\n",
    "    \n",
    "    # constructor to initialize Max Pooling layer with kernel size\n",
    "    def __init__(self, kernel_size):\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "    # generate patches of the input image depending on kernel size\n",
    "    def patches_generator(self, input_image):\n",
    "        height_image, width_image = input_image.shape[0], input_image.shape[1]\n",
    "        self.input_image = input_image\n",
    "        for h in range(height_image):\n",
    "            for w in range(width_image):\n",
    "                patch = input_image[(h*self.kernel_size):(h*self.kernel_size + self.kernel_size), (w*self.kernel_size):(w*self.kernel_size+self.kernel_size)]\n",
    "                yield patch, h, w\n",
    "        \n",
    "    def forward_propogation(self, input_image):\n",
    "        height_image, width_image, num_of_kernel = input_image.shape\n",
    "        max_pooling_output = np.zeros((height_image//self.kernel_size, width_image//self.kernel_size, num_of_kernel))\n",
    "        for patch, h, w in self.patches_generator(self.input_image):\n",
    "            max_pooling_output[h, w] = np.amax(patch, axis=(0, 1))\n",
    "        return max_pooling_output\n",
    "    \n",
    "    def back_propogation(self, dE_dY):\n",
    "        dE_dk = np.zeros(self.input_image.shape)\n",
    "        for patch, h, w in self.patches_generator(self.input_image):\n",
    "            height_image, width_image, num_of_kernel = patch.shape\n",
    "            max_value = np.amax(patch, axis=(0, 1))\n",
    "            for ih in range(height_image):\n",
    "                for iw in range(width_image):\n",
    "                    for k in range(num_of_kernel):\n",
    "                        if patch[ih, iw, k] == max_value[k]:\n",
    "                            dE_dk[h*self.kernel_size+ih, w*self.kernel_size+iw, k] = dE_dY[h, w, k]\n",
    "        return dE_dk\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
